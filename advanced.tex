\chapter{The Advanced FPGA-Friendly Branch Predictors}
\label{chap:advanced}

Chapter~\ref{chap:minimal} proposed gRselect as the best performing minimalistic design that has a hardware budget limited to one M9K BRAM. In this chapter, we relax the constraint on hardware budget and consider more advanced and accurate branch prediction technologies to achieve better performance. Sections~\ref{sec:advanced:perceptron} and~\ref{sec:advanced:tage} discuss Perceptron and TAGE implementations respectively, while Section~\ref{sec:advanced:target} discusses the branch target predictor.


\section{Perceptron Implementation}
\label{sec:advanced:perceptron}
Section~\ref{sec:background:dirpred:perceptron} explained that the Perceptron predictor maintains vectors of weights in a table and produces a prediction through three steps. Each of these steps poses difficulties to map to an FPGA substrate. The rest of this section addresses these problems.

\subsection{Perceptron Table Organization}
\label{sec:advanced:perceptron:table}
Each weight in a perceptron is typically 8-bit wide, and Perceptron predictors usually use at least a 12-bit global history~\cite{perceptron}. The depth of the table, on the other hand, tends to be relatively shallow (e.g., 64 entries for 1KB hardware budget). This requires a very wide but shallow memory, which does not map well to BRAMs on FPGAs. For example, the widest configuration of a M9K BRAM on Altera Stratix IV chips is 36-bit wide times 1K entries~\cite{StratixIVM9K}. If we implement the 1KB Perceptron as proposed by Jim\'enez et al.~\cite{perceptron}, which uses 96-bit wide perceptrons with 12-bit global history, it will result in a huge resource inefficiency as shown in Fig~\ref{fig:perceptronTable}. Stratix IV chips have another larger but slower and fewer M144K BRAM~\cite{StratixIVM9K}, which can be configured as wide as 72-bit times 2K entries, clearly the inefficiency problem persists and it would impact maximum operating frequency.

Since typically the Perceptron table does not require large storage space, the proposed Perceptron implementation uses MLABs as storage, which are fast fine-grain distributed memory resources. Since 50\% of all LABs can be configured as MLAB on Altera Stratix IV devices, using MLABs does not introduce routing difficulty.
\kfig{perceptronTable.pdf}{fig:perceptronTable}{Inefficient use of M9K BRAMs to implement wide but shallow perceptron tables.}{angle = 0, trim = 1in 2in 3.4in 0.6in, clip, width=0.5\textwidth}


\subsection{Multiplication}
\label{sec:advanced:perceptron:mult}
The multiplication stage calculates the products of weights in a perceptron and their global direction histories. Since the value of the global direction history can only be either 1 or -1, the ``multiplication'' degenerates to two cases, i.e., each product can either be the true form or the 2's complement (i.e., negative) form of each weight. A straightforward implementation calculates the negative of each weight and uses a mux to select, using the corresponding global history bit, the appropriate result, as Fig.~\ref{fig:perceptronMult}(a) shows. To improve operating frequency, when updating the perceptron in the execution stage where the branch is resolved, both positive and negative forms of the updated weight can be pre-calculated, and the negatives can be stored on a complement perceptron table. This way, the multiplication stage at prediction time requires only a 2-to-1 mux, as Fig.~\ref{fig:perceptronMult}(b) shows. This optimization trade offs increased resources (it requires extra storage for the negative weights) for improved speed.
\kfig{perceptronMult.pdf}{fig:perceptronMult}{Perceptron multiplication implementation.}{angle = 0, trim = 0.3in 2.2in 3in 0.6in, clip, width=0.7\textwidth}

\subsection{Adder Tree}
\label{sec:advanced:perceptron:adder}
The adder tree sums the products from the multiplication stage. As Section~\ref{sec:eval:advanced:perceptron} will show, a global history of at least 16 bits has to be used to achieve sufficient accuracy. Implementing a 16-to-1 adder tree for 8-bit integers na\"ively degrades maximum frequency severely. The maximum frequency has to be improved for Perceptron to be practical.

This work employs \textit{Low Order Bit (LOB) Elimination} that was proposed by Aasaraai et al.~\cite{lob}. LOB\ elimination ignores the Low Order Bits (LOBs) of each weight and only use the High Order Bits (HOBs) during prediction, while still using all the bits for updates. Section~\ref{sec:eval:advanced:perceptron} shows that eliminating five LOB bits reduces accuracy by less than 1\% compared to using all eight bits, but summing fewer bits results in 14.6\% higher maximum frequency. Section~\ref{sec:eval:advanced:perf} will show that using three HOB for prediction achieves the best overall performance.

Cadenas et al. proposed a method to rearrange the weights stored in the table in order to reduce the number of layers of the adder tree~\cite{perceptronRearrange}. Assuming a Perceptron predictor uses $h$ history bits, instead of storing $h$ weights $w_i$ where $i = 1 ... h$, a new form of weights $\widetilde{w}_i$: $\widetilde{w}_i = - w_i + w_{i+1}; \widetilde{w}_{i+1} = - w_i - w_{i+1},$ for $i = 1, 3, ..., h-1$ is used. The perceptron prediction can now be computed by $y = w_0 + \sum_{i=1}^{h/2}(-G_{2i-1})\widetilde{w}_{2i-h_{2i-1}\bigoplus h_{2i}}$. This work applies this new arrangement because it pushes part of the calculation to the less time critical update logic of the Perceptron predictor so that only $h/2$ additions have to be performed, hence reduces the number of adders required by 50\%. Table~\ref{tab:perceptronArrangement} gives an example of this new arrangement for $i = 1$.

\begin{table}[h]
\begin{center}
\def\arraystretch{1.5} 
\begin{tabular} {|c c|c|}
\hline
\boldmath{$h_1$} & \boldmath{$h_2$} & \boldmath{$(-G_{2i-1})\widetilde{w}_{2i-h_{2i-1}\bigoplus h_{2i}}$}~\textbf{calculation for} $i=1$ \\ \hline
0 & 0 & $(-G_1)\widetilde{w}_{2-0} = \widetilde{w}_2 = -w_1-w_2$\\ \hline
0 & 1 & $(-G_1)\widetilde{w}_{2-1} = \widetilde{w}_1 = -w_1+w_2$\\ \hline
1 & 0 & $(-G_1)\widetilde{w}_{2-1} = -\widetilde{w}_1 = w_1-w_2$\\ \hline
1 & 1 & $(-G_1)\widetilde{w}_{2-0} = -\widetilde{w}_2 = w_1+w_2$\\ \hline
\end{tabular}
\caption{Perceptron Weight Arrangement Example ($i=1$).\label{tab:perceptronArrangement}}
\end{center}
\end{table}

This new arrangement pushes part of the calculation to the less time critical update logic of the Perceptron predictor so that only $h/2$ additions have to be performed, hence reduces the number of adders required by 50\%. However, if we look at its implementation closely, this new arrangement \textit{selects} whether the sum or the difference of original weights $w_i$ and $w_{i+1}$, where $i = 1,3,5,...h-1$, should be calculated. That is, the $h/2$ adders saved during prediction are \textit{replaced} with the same number of multiplexers. The latency difference between 3-bit 2-to-1 muxes and 3-bit adders are negligibly small, which seems to trivialize the usefulness of this arrangement. However, as Section~\ref{sec:advanced:perceptron:structure} will show, this layer of muxes combined with the muxes shown in Fig.~\ref{fig:perceptronMult} map well on FPGAs, therefore it still proves to be beneficial to use this arrangement.

Using fast adders such as carry-lookahead adders does not help to reduce the adder tree latency as the problem is not summing few very wide numbers, but many narrow numbers. Most of the latency comes from going through layers of adders rather than propagating the carry bits. To further improve maximum frequency, this work adapts the implementation of a Wallace Tree~\cite{wallacetree}. A Wallace tree is a hardware implementation of a digital circuit that efficiently sums the partial products when multiplying two integers, which is similar to what  is needed in Perceptron. The Wallace tree implementation proves to be 10.5\% faster than a na\"ive binary reduction tree implementation. 

\subsection{Perceptron Predictor Structure on FPGA}
\label{sec:advanced:perceptron:structure}
Fig.~\ref{fig:perceptronStructure}(a) shows the Perceptron structure with the optimizations introduced earlier in this section. The positive and negative weights with the new arrangement are read from the weight table and the complement weight table. The history bits $h_i$ and $h_{i+1}$ are XORed to select between weights $\widetilde{w}_i$ and $\widetilde{w}_{i+1}$, where $i = 1,3,5,...h-1$. They represent either the sum or the difference of the original weights $w_i$ and $w_{i+1}$, where $i = 1,3,5,...h-1$. These weights are fed into the first layer of muxes, where either the sum or the difference is selected. Then another layer of muxes determine the signs of the sums and/or differences. The outcomes are passed to the Wallace Tree to calculate the final result.

Modern FPGAs such as Altera Stratix family~\cite{StratixIV} and Xilinx Virtex family~\cite{virtex} feature full 6-input Lookup Tables (6-LUT) that can implement any 6-input function. The two layers of muxes shown in Fig.~\ref{fig:perceptronStructure}(a) can be combined into one layer of 4-to-1 muxes, as shown in Fig.~\ref{fig:perceptronStructure}(b). Each 4-to-1 multiplexer is a 6-input function, hence can be implemented with a single 6-LUT. Therefore, the main benefit of replacing the layer of adders with the layer of muxes is that these muxes can be combined with the other layer of muxes. This latency reduction comes for free from the arrangement discussed in Section~\ref{sec:advanced:perceptron:adder}.
\kfig{perceptronStructure.pdf}{fig:perceptronStructure}{Perceptron Structure on FPGA.}{angle = 0, trim = 0in 0in 0in 0in, clip, width=0.9\textwidth}


\section{TAGE Implementation}
\label{sec:advanced:tage}
Section~\ref{sec:eval:advanced:comparison} shows that TAGE  is the most accurate amongst all the direction predictors considered in this work when they use the same hardware budget. Unfortunately, TAGE uses multiple tables with tagged entries that require comparator driven logic which does not map well onto FPGAs. Section~\ref{sec:eval:advanced:perf} shows that the resulting frequency slowdown with TAGE is not amortized by the corresponding accuracy gains. Fortunately, TAGE can be used as an overriding predictor maintaining the accuracy gains and relatively high operating frequency.

The critical path of TAGE is as follows: (1)~It performs an elaborate PC-based hashing to generate multiple table indices, one per table. (2)~It accesses the tables and in parallel compares the tags of the read entries to determine whether they match. (3)~Finally each matching entry has to pass through cascaded layers of multiplexers to select the longest matching prediction. Although the latency of these operations is high, the path can be easily pipelined to achieve much higher operating frequency. Based on this observation, this work explores an overriding branch predictor implementation using TAGE.

Overriding branch prediction is a technique to leverage the benefits of both fast but less accurate, and slow but more accurate predictors. This technique has been used commercially, e.g., in the Alpha EV8 microprocessors~\cite{alphaEV8}. In an overriding predictor, a faster but less accurate base predictor makes a base prediction quickly in the first cycle, and then a slower but more accurate predictor overrides that decision, at a latter cycle, if it disagrees with the base prediction. 

In this work, the base predictor is the simple bimodal predictor included in TAGE itself, i.e., $T_0$ in Fig.~\ref{fig:tage}. The bimodal predictor provides a base prediction in the first cycle, and the tagged components of the original TAGE provide a prediction at the second cycle. Sections~\ref{sec:eval:advanced:comparison} and~\ref{sec:eval:advanced:fmax} show that an overriding TAGE predictor outperforms all the other branch prediction schemes in terms of both accuracy and maximum frequency.

With an overriding predictor, there is no guarantee that the overriding component will indeed be correct. Accordingly, it is essential that any benefits gained when the overriding component is right are higher than the performance lost when it is wrong. For this purpose, this work proposes the use of a confidence mechanism for applying overrides judiciously. Specifically, the confidence mechanism implemented is a small table with 256 entries that is indexed by eight bits from the PC. Each entry is a 10-bit saturating counter.  The counter is updated whenever the basic and the overriding component disagree. When they disagree, the counter is incremented when the overriding component is correct and reset otherwise. Overrides are activated only after the counter saturates. Seznec also suggested using a similar confidence mechanism, a \textit{statistical corrector}, in his ISL-TAGE improvement over the original TAGE~\cite{isltage}.  There, the statistical corrector is used in a single-cycle non-overriding TAGE predictor to avoid using the tagged components whenever the bimodal component proves better.  Seznec's observation was that the tagged components fail at predicting branches that are statistically biased towards a direction but not correlated to the history path. On some of these branches, TAGE often performs worse than a simple PC-indexed table, e.g., a bimodal predictor.

The confidence mechanism/statistical corrector used in this work is similar to that proposed by Jacobsen et al.~\cite{confidence}, except that the specific statistical corrector in this work is only updated when the basic and the overriding component disagree. The specific confidence mechanism performed better than Seznec's mechanism. This is no surprise as here it is used to guide overrides in an overriding TAGE predictor. Specifically, in Nios~II-f where the branch resolution latency is only two cycles, the overriding TAGE saves one cycle for each correct override, but loses two for each incorrect override.
%Hence, the overriding TAGE must make two right decisions for every one wrong decision to break even. 
Hence, the overriding TAGE must be very confident to make an overriding decision, which necessitates the specific statistical corrector.

As a result, this work proposes four TAGE-based designs that use one or two cycles, with or without a confidence mechanism:  (1)~the single-cycle TAGE, which requires TAGE to provide a prediction in one cycle (i.e., in the fetch stage), (2)~the Overriding TAGE (\mbox{O-TAGE}), which uses just the bimodal predictor (i.e., $T_0$) to provide a base prediction in the first cycle, and \textit{always} overrides the base prediction if TAGE disagrees at the end of the second cycle, (3)~the single-cycle TAGE with a Statistical Corrector (single-cycle \mbox{TAGE-SC}), which forces the predictor to use the base prediction unless TAGE consistently disagrees over several encounters of the same event, and (4)~the Overriding TAGE with a Statistical Corrector (\mbox{O-TAGE-SC}), which is similar to the single-cycle \mbox{TAGE-SC} except that TAGE overrides the base prediction in the second cycle. Fig.~\ref{fig:tageconfig} demonstrates these TAGE-based designs, the target predictor is not shown in these figures for simplicity. The accuracy and critical path of the Perceptron predictor did not justify investigating an overriding configuration based on Perceptron.

\kfig{tageconfig.pdf}{fig:tageconfig}{The four TAGE-based designs: (a) single-cycle TAGE, (b) single-cycle TAGE-SC, (c) O-TAGE, and (d) O-TAGE-SC.}{angle = 0, trim = 0in 0in 2.4in 0in, clip, width=0.9\textwidth}

The specific configuration parameters are summarized in Table.~\ref{tab:tageconfig}. 

\begin{table}[h]
\begin{center}
\begin{tabular} {|c|c|}
\hline
\textbf{Structure} & \textbf{Storage} \\ \hline
$T_0$ & 20 Kbits\\
$T_1$ and $T_2$ & 12 Kbits each\\
$T_3$ and $T_4$ & 26 Kbits each\\
$T_5$ & 28 Kbits\\
$T_6$ & 30 Kbits\\
$T_7$ & 16 Kbits\\
$T_8$ and $T_9$ & 17 Kbits each\\
$T_{10}$ & 18 Kbits\\
$T_{11}$ & 9.5 Kbits\\
$T_{12}$ & 10 Kbits\\ \hline
\textbf{Total} & 241.5Kbits \\ \hline
\end{tabular}
\caption{Sizes of Tables $T_i$ in the four TAGE based predictors.}\label{tab:tageconfig}
\end{center}
\end{table}


\section{Branch Target Predictor}
\label{sec:advanced:target}
Chapter~\ref{chap:minimal} has shown that when using one M9K BRAM -- a hardware budget on par with that of Nios~II-f -- eliminating the BTB and using \textit{Full Address Calculation} (FAC) together with a RAS results in better performance~\cite{grselect}. It has also shown that direct branches and returns comprise over 99.8\% of all branches. Implementing FAC with RAS can cover these branches with 100\% accuracy, therefore having a BTB to cover all branches results in negligible improvement in target prediction accuracy.

Since, this chapter investigates how branch prediction accuracy can improve when additional hardware resources are used, adding a BTB for better target prediction coverage could improve target prediction accuracy. Accordingly, we consider reintroducing a BTB. However, simulations show that accuracy is still better without a BTB\footnotemark\footnotetext{These simulation resutls are not included in this thesis.}. This is because when the target predictor only has FAC and RAS, it never predicts indirect branches that are not returns because it is not capable to do so. As a result, the destructive aliasing in the \textit{direction} predictor is alleviated because fewer branches are being predicted. Based on this observation, the Perceptron and TAGE predictors in this chapter also uses FAC with a 16-entry RAS as the branch target predictor.



