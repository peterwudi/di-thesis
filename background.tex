\chapter{Background}
\label{chap:background}
This chapter reviews branch prediction basics and relevant past work. Section~\ref{sec:background:whybp} introduces the importance of branch prediction. Section~\ref{sec:background:bpoverview} describes the structure of a canonical branch predictor. Sections~\ref{sec:background:target} and~\ref{sec:background:dirpred} overviews the branch prediction schemes considered in this thesis. Finally, Section~\ref{sec:background:fpga} briefly introduces Field Programmable Gate Arrays (FPGAs) and soft processors.

\section{Why Branch Prediction?}
\label{sec:background:whybp}
In modern microprocessors, instruction pipelining is a key feature to improve performances. At each cycle, a new instruction is read from the memory at the location indicated by the program counter (PC). The pipeline works ideally when there is no transfer of control, i.e., PC is incremented by a constant at each cycle to fetch the next instruction assuming a fixed instruction length, which is true for Altera's Nios~II instruction set. However, when there is a transfer of control (e.g., a taken branch instruction), the next instruction to fetch is unknown at this point. The processor has to stall the pipeline to wait until the branch is \textit{resolved}, that is to determined its target address, to fetch the correct instruction, hence severely impacts performance.

Branch prediction is a technique to improve performance by guessing the branch target  to allow the processor to fetch the corresponding instructions and execute them speculatively. The processor fetches the instruction at the predicted target, and compares the predicted with the actual target when the branch is resolved several cycles later. If the prediction was correct, the processor fetched the correct instructions without stalling the pipeline, as if the instruction stream were not disrupted. On the other hand, if the prediction was incorrect, the processor fetched wrong instructions, and the intermediate results produced by these wrong instructions has to be squashed with an extra roll-back penalty. Therefore, prediction accuracy is the key to high performance.

\section{Branch Prediction Overview}
\label{sec:background:bpoverview}
Branch prediction entails two steps: \textit{direction} prediction and \textit{target} prediction. Branch direction prediction predicts whether a branch instruction will be taken or not, while branch target prediction predicts the target instruction address if the branch is taken.

Fig.~\ref{fig:bpcanonical} shows the organization of a typical branch predictor comprising: (1)~a direction predictor (DIR), (2)~a Branch Target Buffer (BTB), and (3)~a Return Address Stack (RAS).  

The predictor operates in the fetch stage where it aims to predict the program counter (PC), i.e., the memory address, of the instruction to fetch in the next cycle using the current instruction's PC and other dynamically collected information. The DIR guesses whether the branch will be taken or not. The BTB and the RAS guess the address for ``predicted as taken" branches and function returns respectively. The multiplexer at the end selects based on the branch type and the direction prediction whether the target is the fall through address (PC+4 in Nios~II), the target predicted by the BTB, or the target provided by the RAS. Since, at this point in time, the actual instruction is not available in a typical ASIC implementation, it is not directly possible to determine whether the instruction is a return, a branch, or some other instruction. Accordingly, a Selection Logic block uses either pre-decode information or a PC-based, dynamically populated lookup table to guess which target is best to use. With the latter scheme, when no entry exists in the lookup table, some default action is taken until the first time a branch is encountered. Once the branch executes, its type is stored in the lookup table where it serves to identify the branch type on subsequent encounters. This scheme is not perfectly accurate due to aliasing.
\kfig{bpcanonical.pdf}{fig:bpcanonical}{Canonical Branch Predictor.}{angle = 0, trim = 1in 1in 0.5in 0.8in, clip, width=0.6\textwidth}

\section{Branch Target Prediction}
\label{sec:background:target}
Branch Target Prediction usually requires a Branch Target Buffer (BTB), a cache-like structure that records the addresses of the branches and their target addresses. If a branch is predicted to be taken and there is also a BTB hit, then the next PC is set to be the predicted target. A BTB can be set-associative to reduce aliasing.

Another common structure used for branch target prediction is the Return Address Stack (RAS),  a stack-like structure that predicts the target address of function returns. When a call instruction executes, the return address of that call is pushed onto the RAS. When the processor executes the corresponding return instruction, RAS pops the return address and provides a prediction. The prediction is accurate as long as the size of the RAS is greater than the current call depth. Most modern processors have a shallow RAS because typical programs generally do not have very deep call depths.


\section{Branch Direction Prediction}
\label{sec:background:dirpred}
This section describes the branch direction predictors. This section first briefly reviews \textit{static} prediction schemes. Then, it reviews \textit{dynamic} prediction schemes, starting with the simple schemes such as bimodal~\cite{bimodal}, gshare and gselect~\cite{McFarling}, to more complex schemes such as Perceptron~\cite{perceptron} and TAGE~\cite{tage}.

\subsection{Static Prediction Scheme}
\label{sec:background:dirpred:static}
Static branch direction predictors statically predict a branch to be taken or non-taken. Initially, this strategy was motivated on the observation that most branches are taken. However, prior work has shown that approximately 30\% of all branches are \textit{unconditional} branches (e.g., functions calls and returns)~\cite{histogram}, which are of course taken. The remaining 70\% branches are \textit{conditional} branches, and their directions may not be strongly biased towards one direction than the other.

The main weakness of static branch predictors is that they cannot adapt to various applications. A static prediction scheme variation called ``backward-taken forward-not-taken" takes advantage of the observation that most backward branches are taken (e.g., the end of loops), while forward branches are often not taken. However, the improvement in accuracy is not as significant. As the pipeline in modern processors became deeper, the misprediction penalties increased, which necessitated more accurate \textit{dynamic} branch predictors.

\subsection{Bimodal, Gshare and Gselect Predictor}
\label{sec:background:dirpred:bimodal}
Bimodal~\cite{bimodal}, Gshare and Gselect~\cite{McFarling} are some of the simplest dynamic direction prediction schemes. Fig.~\ref{fig:simpleDIR} shows the organization of these three predictors. Each of these predictors has a \textit{Pattern History Table} (PHT)~\cite{pht}. The entries of a PHT are called \textit{2-bit saturating counters}, which are used to record the directions of the previously seen branches. The 2-bit saturating counters are incremented/decremented when their corresponding branch is taken/not taken, and the high order bit is used as direction prediction.

The PHT in the bimodal predictor is indexed by selected bits of the PC. Accordingly, the bimodal predictor works by identifying the temporal bias of each branch instruction. Since not all bits of the PC are used to index the PHT, multiple different branches can be assigned to the same 2-bit saturating counter. This \textit{aliasing} can be destructive to the direction records, hence degrades performance.

While many branches exhibit temporal biases which bimodal predictor can capture successfully, often there are branches that exhibit specific direction patterns or branches that are correlated with one another. Gshare and gselect take advantage of these phenomena and try to avoid aliasing. Specifically, to improve accuracy they utilize a structure called a \textit{Global History Register} (GHR). The GHR is a shift register that stores recently resolved branch directions. The only difference between bimodal, gshare and gselect is the indexing of the PHTs. Bimodal uses partial PC, gshare uses partial PC XORed with GHR, and gselect uses partial PC concatenated with GHR. Gshare and gselect have better accuracy over bimodal because of the reduced aliasing by correlating local history (i.e., PC) with global history (i.e., GHR).
They essentially record a series of branch directions with the direction of a subsequent branch. This allows them  to discover patterns in branch behavior that may not be amenable to temporal bias based prediction.

\kfig{simpleDIR.pdf}{fig:simpleDIR}{Bimodal, Gshare and Gselect.}{angle = 0, trim = 0.6in 1.1in 0.5in 1.3in, clip, width=0.6\textwidth}


\subsection{Perceptron Predictor}
\label{sec:background:dirpred:perceptron}
The Perceptron predictor uses vectors of signed weights (i.e., perceptrons) to represent correlations among branch instructions~\cite{perceptron}. It is capable of determining which preceding branches are correlated with the current branch contrary to Gshare and Gselect that correlate with all preceding branches in the history register. Fig.~\ref{fig:perceptron} shows the structure of a Perceptron predictor. It produces a prediction through the following steps: (1)~A \textit{perceptron} is read from the table. (2)~The weights are multiplied with factors chosen based on the  corresponding global history bits. The weights are multiplied by 1 for taken and -1 for not-taken. (3)~The resulting products are summed up and a prediction is made based on the sign of the result; predict taken if the sum is positive, and not-taken otherwise. Formally, for a Perceptron predictor using $h$ history bits, let $G_i$, where $i = 1...h$, be 1 for taken and -1 for not-taken, each weight vector has $h$ weights $w_{0...h}$, where the bias constant $w_0 = 1$. The predictor has to calculate $y = w_0 + \sum_{i=1}^{h} G_iw_i$, and predict taken if $y$ is positive and not-taken otherwise.
\kfig{perceptron.pdf}{fig:perceptron}{The Perceptron branch predictor.}{angle = 0, trim = 0.8in 3.5in 2in 0.8in, clip, width=0.6\textwidth}

When a branch is resolved, the corresponding perceptron is updated. Let $y$ be the computed results at prediction time, $t$ be the branch outcome, $x_i$ be the $i_{th}$ global history, and let $\theta$ be the \textit{threshold}, the following algorithm is used to train the Perceptron:\vspace{8 mm}

\begin{spacing}{1} 
\texttt{if sign(y) $\neq\ t$ or $\left| y \right| \leq \theta$}

\texttt{\hspace{8mm} for i = 0 to n do}

\texttt{\hspace{16mm} $w_i = w_i + tx_i$}

\texttt{\hspace{8mm} end for}

\texttt{end if}
\end{spacing}
\vspace{8 mm}

In the preceding algorithm, $t$ and $x_i$ are bipolar, i.e., each of them is $1$ if the branch outcome is taken and $-1$ otherwise. The algorithm increments the $i_{th}$ weight when the outcome agrees with $x_i$, and decrements the weight otherwise. When there is a strong correlation, the absolute value of the weight will be large, which will have a large influence on the outcome. On the other hand, when there is a weak correlation, the absolute value of the weight will be close to zero, hence contributes little to the result.


\subsection{Tagged Geometric History Length Branch Predictor (TAGE)}
\label{sec:background:dirpred:tage}
The TAGE predictor features a bimodal predictor as a base predictor $T_0$ and a set of $M$ tagged predictor components $T_i$~\cite{tage}. These tagged predictor components $T_i$, where $1\leq i\leq M$, are indexed with hash functions of the branch address and the global branch/path history of various lengths. The global history lengths used for computing the indexing functions for tables $T_i$ form a geometric series, i.e., $L(i) = (int)(\alpha^{i{}^{\_}1}\times L(1)+0.5)$. TAGE achieves its high accuracy by utilizing very long history lengths judiciously.
Essentially, the base predictor captures the bulk of branches that tend to be biased, while the remaining components capture exceptions by recording specific history events that lead to exceptions that foil the base predictor.
Fig.~\ref{fig:tage} shows a 5-component TAGE predictor. Each table entry has a 3-bit saturating counter \textit{ctr} for the prediction result, a \textit{tag}, and a 2-bit useful counter \textit{u}. The table indices are produced by hashing the PC and the global history using different lengths per table $L(i)$. All tables are accessed in parallel and each table provides a valid prediction only on a tag match and provided that the corresponding useful counter is saturated. The final prediction comes from the matching tagged predictor component that uses the longest history.
\kfig{tage.pdf}{fig:tage}{A 5-component TAGE branch predictor.}{angle = 0, trim = 0.6in 0.6in 0.4in 0.2in, clip, width=0.9\textwidth}

Seznec et al. defines the \textit{provider component} as the predictor component that provides the prediction, and the alternate prediction \textit{altpred} as the prediction that would have occurred without the provider component~\cite{tage}.

When updating TAGE, the useful counter $u$ of the provider component is updated when the alternate prediction disagrees with the provider. $u$ is incremented if the provider is correct, and decremented otherwise.

The prediction counter $ctr$ of the provider is incremented if the prediction is correct and decremented otherwise. If the prediction from the provider $T_i$, where $i < M$, is incorrect, a new entry will be allocated on a predictor component $T_k$ that uses a longer history, i.e., $i<k\leq M$.


\section{Field Programmable Gate Arrays and Soft Processors}
\label{sec:background:fpga}
Field Programmable Gate Arrays (FPGAs) are chips that can be reconfigured by designers. Modern FPGAs such as Altera's Stratix IV~\cite{StratixIV} have large number of logic blocks connected by reconfigurable interconnects. The logic blocks usually consist of Look-up Tables (LUTs), registers and Block RAMs (BRAMs) etc. Designers program digital circuits with Hardware Description Languages (HDLs) such as Verilog and VHDL, and then use CAD tools provided by the FPGA vendors to synthesize the design.

FPGAs can be reconfigured to fit specific requirements of various applications, therefore they are popular choices for hardware acceleration. Due to the increasing cost and time of designing ASIC, an increasing number of embedded systems are being built using FPGA platforms~\cite{softprocessor}. These systems usually contain one or more embedded processors, which necessitates high-performance FPGAs-based \textit{soft processors}. 

Since an FPGA platform is significantly different than an ASIC, hard and soft processors have distinct design tradeoffs. Commercial soft processors such as Alteras's Nios~II-f~\cite{niosiif} and Xilinx's MicroBlaze~\cite{microblaze} usually have shallow pipelines, mostly because the relative speed gap between memory and logic is much smaller than on an ASIC. Accordingly, branch predictor designs for soft processors also have to be re-evaluated.

The following chapters discuss various FPGA-friendly branch predictor designs.

