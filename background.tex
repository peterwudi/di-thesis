\chapter{Background and Goals}
\label{chap:background}
This chapter covers the background of branch prediction and the goal of this thesis. 

\section{Why Branch Prediction?}
\label{sec:background:whybp}
In modern microprocessors, instruction pipelining is a key feature to improve performances. At each cycle, a new instruction is read from the memory at the location indicated by the program counter (PC). The pipeline works ideally when there is no transfer of control, i.e., PC is incremented by a constant at each cycle to fetch the next instruction. However, when there is a transfer of control (e.g. a taken branch instruction), the next instruction to fetch is unknown at this point. The processor has to stall the pipeline to wait until the branch is resolved to fetch the correct instruction, hence severely impacts performance.

Branch prediction is a technique to improve performance by guessing the branch target (i.e. the next instruction) to allow the processor to run speculatively. The processor fetches the predicted target, and compares the prediction with the actual target when the branch is resolved several cycles later. If the prediction was correct, the processor fetched the correct instructions without stalling the pipeline, as if the instruction stream were not disrupted. On the other hand, if the prediction was incorrect, the processor fetched wrong instructions, and the intermediate results related to those wrong instructions has to be squashed, possibly with an extra maintenance penalty. Therefore, prediction accuracy is the key to high performance.

\section{Branch Prediction Scheme}
\label{sec:background:bpscheme}
Branch prediction has two aspects, i.e., direction prediction and target prediction. Branch direction prediction predicts whether a branch instruction will be taken or not, while branch target prediction predicts the target instruction address if the branch is taken.

Fig.~\ref{fig:bpcanonical} shows the organization of a typical branch predictor comprising: (1)~a direction predictor (DIR), (2)~a Branch Target Buffer (BTB), and (3)~a Return Address Stack (RAS).  

The predictor operates in the fetch stage where it aims to predict the program counter (PC), that is the address in memory, of the instruction to fetch in the next cycle using the current instruction's PC and other dynamically collected information. 

The DIR guesses whether the branch will be taken or not. The BTB and the RAS guess the address for  predicted as taken branches and function returns respectively. The multiplexer at the end selects based on the branch type and the direction prediction whether the target is the fall through address (PC+4 in Nios~II), the target predicted by the BTB, or the target provided by the RAS. Since, at this point in time, the actual instruction is not available in a typical ASIC implementation, it is not directly possible to determine whether the instruction is a return, a branch, or some other instruction. Accordingly, a Selection Logic block uses either pre-decode information or a PC-based, dynamically populated lookup table to guess which target is best to use. With the latter scheme, when no entry exists in the lookup table, some default action is taken until the first time a branch is encountered. Once the branch executes, its type is stored in the lookup table where it serves to identify the branch type on subsequent encounters. This scheme is not perfectly accurate due to aliasing.
\kfig{bpcanonical.pdf}{fig:bpcanonical}{Canonical Branch Predictor.}{angle = 0, trim = 1in 1in 0.5in 0.8in, clip, width=0.6\textwidth}

\section{Branch Target Predictors}
\label{sec:background:target}
Branch Target Prediction usually requires a Branch Target Buffer (BTB), a cache-like structure that records the addresses of the branches and the target addresses associated with them. If a branch is predicted to be taken and there is also a BTB hit, then the next PC is set to be the predicted target. A BTB can also have set-associativity to reduce the impact from aliasing.

Another common structure for branch target prediction is a Return Address Stack (RAS). It is a stack-like structure that accurately predicts the target address of function returns. When a call instruction executes, the return address of that call is pushed on RAS. When the processor executes the corresponding return instruction, RAS pop the return address and always provides the accurate prediction. Most modern processors have a shallow RAS because typical programs generally do not have very deep call depths. RAS will fail to provide correct target prediction if it is overflowed.


\section{Branch Direction Predictors}
\label{sec:background:dirpred}
This section describes the branch direction predictors considered in this thesis.

\section{Static Predictor}
\label{sec:background:dirpred:static}
Static branch direction predictors statically predicts a branch to be taken or non-taken. This strategy is based on the observation that most branches are taken. The main weakness of static branch predictors is its inability to adapt to various applications. There are some other variations of static branch predictors such as ``backward-taken forward-not-taken". However, the improvements in accuracy is not significant. As the pipeline in modern processors becomes deeper, the misprediction penalties increases, which necessitates more accurate \textit{dynamic} branch predictors.

\subsection{Bimodal, Gshare and Gselect Predictor}
\label{sec:background:dirpred:bimodal}
Bimodal, Gshare and Gselect~\cite{McFarling} are some of the simplest dynamic direction prediction schemes. Fig.~\ref{fig:simpleDIR} shows the organization of these three predictors. Each of these predictors has a \textit{Pattern History Table} (PHT). The entries of a PHT are called \textit{2-bit saturating counters}, which are used to record the directions of the previously seen branches. The 2-bit saturating counters are incremented/decremented when their corresponding branch is taken/not taken, and the high order bit is used as direction prediction.

The PHT in bimodal is indexed by selected bits of PC. Since not all bits of PC are used to index the PHT, multiple different branches can be assigned to the same 2-bit saturating counter. This \textit{aliasing} can be destructive to the direction records, hence degrades performance.

Gshare and gselect utilize a structure called a \textit{Global History Register} (GHR). The GHR is a shift register that stores recently resolved branch directions. The only difference between bimodal, gshare and gselect is the indexing of the PHTs. Bimodal uses partial PC, gshare uses partial PC XORed with GHR, and gselect uses partial PC concatenated with GHR. Gshare and gselect has better accuracy over bimodal because it reduces aliasing by correlating local history (i.e. PC) and global history (i.e. GHR).
\kfig{simpleDIR.pdf}{fig:simpleDIR}{Bimodal, Gshare and Gselect.}{angle = 0, trim = 0.6in 1.1in 0.5in 1.3in, clip, width=0.6\textwidth}


\subsection{Perceptron Predictor}
\label{sec:background:dirpred:perceptron}
The perceptron predictor use vectors of weights (i.e., perceptrons) to represent correlations among branch instructions~\cite{perceptron}. Fig.~\ref{fig:perceptron} shows the structure of a perceptron predictor.

When making a prediction, a weight vector is loaded from the table, and each weight is multiplied by the corresponding global branch history (i.e. 1 if taken and -1 if not-taken) from the GHR. Finally, the multiplication results are summed up, and perceptron predicts taken if the sum is positive, and not taken otherwise.
\kfig{perceptron.pdf}{fig:perceptron}{The preceptron branch predictor.}{angle = 0, trim = 1in 3.5in 2in 1.5in, clip, width=0.6\textwidth}


\subsection{TAgged GEometric history length Branch Predictor (TAGE)}
\label{sec:background:dirpred:tage}
The TAGE predictor features a bimodal predictor as base predictor $T_0$ to provide a basic prediction and a set of $M$ tagged predictor components $T_i$~\cite{tage}. These tagged predictor components $T_i$, where $1\leq i\leq M$, are indexed with hash functions of the branch address and the global branch/path history with various length. The global history lengths used for computing the indexing functions for tables $T_i$ form a geometric series, i.e., $L(i) = (int)(\alpha^{i{}^{\_}1}\times L(1)+0.5)$. TAGE achieves its high accuracy by utilizing very long history lengths.

Fig.~\ref{fig:tage} shows a 5-component TAGE predictor. Each table entry has a 3-bit saturating counter \textit{ctr} for prediction result, a \textit{tag}, and a 2-bit useful counter \textit{u}. The indices of the tables are produced by hashing PC and global history with various lengths. A valid prediction result from each table is provided only on a tag match (i.e. a hit). The final prediction of TAGE comes from the hitting tagged predictor component that uses the longest history.
\kfig{tage.pdf}{fig:tage}{A 5-component TAGE branch predictor.}{angle = 0, trim = 0.6in 0.6in 0.4in 0.2in, clip, width=0.9\textwidth}


\section{Field Programmable Gate Arrays (FPGAs) and Soft Processors}
\label{sec:background:fpga}
Field Programmable Gate Arrays (FPGAs) are chips that can be reconfigured by designers. Modern FPGAs such as Altera's Stratix IV~\cite{StratixIV} have large number of logic blocks connected by reconfigurable interconnects. The logic blocks usually consist of Look-up Tables (LUTs), registers and Block RAMs (BRAMs) etc. Designers program digital circuits with Hardware Description Languages (HDLs) such as Verilog and VHDL, and then use CAD tools provided by the FPGA vendors to synthesize the design.

FPGAs can be reconfigured to fit specific requirements of various applications, therefore they are popular choices for hardware acceleration. Due to the increasing cost and time of designing ASIC, an increasing number of embedded systems are being built using FPGA platforms~\cite{softprocessor}. These systems usually contain one or more embedded processors, necessitates high-performance FPGAs-based \textit{soft processors}. 

Since an FPGA platform is significantly different than ASIC, hard and soft processors have distinct design tradeoffs. Commercial soft processors such as Alteras's Nios~II-f~\cite{niosiif} and Xilinx's MicroBlaze~\cite{microblaze} usually have shallow pipelines, mostly because the relative speed gap between memory and logic is much smaller than on ASIC. Accordingly, branch predictor designs for soft processors also have to be re-evaluated.


\section{Design Goals}
\label{sec:background:goal}

This work aims to design a branch predictor that (1)~balances operating frequency and accuracy to maximize execution performance, and (2)~uses as few on-chip resources as possible.

This work proposes a minimalistic branch predictor for Altera's highest performing soft-processor Nios~II-f. Since Nios~II-f uses three BRAMs in total and only one BRAM is used for branch prediction~\cite{niosiif}, the proposed minimalistic branch predictor has a limited resource budget of one BRAM; each additional BRAM would represent a more than 1/3 overhead in terms of BRAM resources. The design of the minimalistic predictor considers the most commonly used direction predictors, bimodal, gshare and gselect. These predictors use a single lookup table and map relatively well onto a single BRAM.

This work also includes an investigation of the state-of-the-art perceptron and TAGE predictors. These more elaborated branch predictors are generally more accurate, but often cannot be accessed within a single cycle. Therefore, this work considers an overriding predictor that uses a simple bimodal predictor to provide base prediction, and then a more accurate predictor to override the base prediction if it disagrees with the bimodal predictor.


